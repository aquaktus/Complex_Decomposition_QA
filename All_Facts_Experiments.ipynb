{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_utils import Claim_Verification_Dataset\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from src.QA_models import T5_QA_From_Oracle_Facts, T5_QA_Only_Query\n",
    "from src.QA_models import *\n",
    "from main import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_verification_dataset = Claim_Verification_Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = claim_verification_dataset.get_data('rule_taker_3', 'train')\n",
    "dev_data = claim_verification_dataset.get_data('rule_taker_3', 'dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Dave is not smart.',\n",
       " 'facts': ['Anne is quiet.',\n",
       "  'Dave is round.',\n",
       "  'Fiona is quiet.',\n",
       "  'Dave is rough.',\n",
       "  'Dave is smart.',\n",
       "  'Fiona is not round.',\n",
       "  'Bob is kind.',\n",
       "  'Dave is not young.',\n",
       "  'Anne is not young.',\n",
       "  'Bob is young.',\n",
       "  'Kind, young things are not smart.'],\n",
       " 'answer': False}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claim_verification_dataset.data['rule_taker_1']['train'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(\"--name test --batch_sz 32 --epochs 5 --warmup_steps 200 --gpu_id 1\".split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5_QA_From_Oracle_Facts(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(train_data, batch_size=args.batch_sz, shuffle=True, collate_fn = model.collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "\n",
      "  | Name        | Type                       | Params\n",
      "-----------------------------------------------------------\n",
      "0 | transformer | T5ForConditionalGeneration | 222 M \n",
      "-----------------------------------------------------------\n",
      "222 M     Trainable params\n",
      "0         Non-trainable params\n",
      "222 M     Total params\n",
      "891.614   Total estimated model params size (MB)\n",
      "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:51: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a85d3cce16f14af08cbf624f1dd6cd6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl_trainer = Trainer(gpus=args.gpu_id, gradient_clip_val=0.5, amp_level='O1', max_epochs=args.epochs)\n",
    "model.train()\n",
    "pl_trainer.fit(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Context: Julius Caesar had three children. ||| Genghis Khan had sixteen children. ||| Modern geneticists have determined that  out of every 200 men today has DNA that can be traced to Genghis Khan. ||| Query: Are more people today related to Genghis Khan than Julius Caesar?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sample_to_input_text(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Re-writing: 100%|██████████| 1/1 [00:00<00:00,  3.54it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'qid': '2bc9c4f9c19c167187f2',\n",
       "  'term': 'Genghis Khan',\n",
       "  'description': 'founder and first Great Khan of the Mongol Empire',\n",
       "  'question': 'Are more people today related to Genghis Khan than Julius Caesar?',\n",
       "  'answer': True,\n",
       "  'facts': ['Julius Caesar had three children.',\n",
       "   'Genghis Khan had sixteen children.',\n",
       "   'Modern geneticists have determined that  out of every 200 men today has DNA that can be traced to Genghis Khan.'],\n",
       "  'decomposition': ['How many kids did Julius Caesar have?',\n",
       "   'How many kids did Genghis Khan have?',\n",
       "   'Is #2 greater than #1?'],\n",
       "  'evidence': [[[['Caesarion-2', 'Julia (daughter of Caesar)-1']],\n",
       "    [['Alakhai Bekhi-1', 'Tolui-1'], 'no_evidence'],\n",
       "    ['operation']],\n",
       "   [[['Julius Caesar-75']], [['Genghis Khan-17']], ['operation']],\n",
       "   [[['Gaius Julius Caesar-7']],\n",
       "    [['Genghis Khan-15'], 'no_evidence'],\n",
       "    ['no_evidence', 'operation']]],\n",
       "  'all_generations': ['true', 'false', 'True', 'truth'],\n",
       "  'scores': tensor([0.5528, 0.4312, 0.0110, 0.0050]),\n",
       "  'top_output': 'true'},\n",
       " {'qid': '03caf265939fab701dee',\n",
       "  'term': 'The Police',\n",
       "  'description': 'English rock band',\n",
       "  'question': 'Could the members of The Police perform lawful arrests?',\n",
       "  'answer': False,\n",
       "  'facts': ['The members of The Police were musicians, not law enforcement officers.',\n",
       "   'Only law enforcement officers can perform lawful arrests.'],\n",
       "  'decomposition': ['Who can perform lawful arrests?',\n",
       "   'Are members of The Police also #1?'],\n",
       "  'evidence': [[[['Arrest-2']],\n",
       "    [[\"Citizen's arrest-2\", 'The Police-1'], 'operation']],\n",
       "   [[['Arrest-7', 'Law enforcement officer-13']],\n",
       "    [['Law enforcement officer-13', 'The Police-8']]],\n",
       "   [[['Lawful interception-28']], ['operation']]],\n",
       "  'all_generations': ['false', 'true', 'not false', 'not true'],\n",
       "  'scores': tensor([0.8683, 0.0893, 0.0339, 0.0084]),\n",
       "  'top_output': 'false'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.inference(train_data[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasoning in Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Reasoning_in_Decoder(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[1079,   19, 1692,    1,    0],\n",
       "        [3790,   19, 4459, 1273,    1]]), 'attention_mask': tensor([[1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_ids = tokenizer(['John is blue', 'Mary is yellowish'], return_tensors='pt', padding=True)\n",
    "enc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  363,  3243,    19,  1079,    58,  8178,    19, 32099,     3,     5,\n",
       "             1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_ids = tokenizer(['What colour is John? Jon is <extra_id_0>.'], return_tensors='pt', padding=True)\n",
    "dec_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs = transformer.encoder(input_ids=enc_ids['input_ids'], attention_mask=enc_ids['attention_mask'])\n",
    "encoder_hidden_states = encoder_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 512])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 0]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_ids['attention_mask'][[True, False]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  True, False,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_ids['attention_mask'].reshape(-1).to(torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 512])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = encoder_hidden_states.reshape(-1,512)[enc_ids['attention_mask'].reshape(-1).to(torch.bool)]\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 512])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([a, torch.zeros(3,512)]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = model.collate([{'facts':['The capital of <extra_id_0> is Madrid.'], 'decoder_text':'<pad> Madrid, Madrid'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'example_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-55d7fed137d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'example_set' is not defined"
     ]
    }
   ],
   "source": [
    "batch = model.collate(example_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fusion_map': [[0, 1]],\n",
       " 'encoder_ids': tensor([[   37,  1784,    13, 32099,    19, 12033,     5,     1]]),\n",
       " 'encoder_att_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " 'decoder_input_ids': tensor([[    0, 12033,     6, 12033]]),\n",
       " 'decoder_target_ids': tensor([[12033,     6, 12033,     1]]),\n",
       " 'decoder_att_mask': tensor([[1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_enc_hidden_states = encoder_hidden_states.reshape(1,-1,512)\n",
    "flat_enc_atts = enc_ids['attention_mask'].reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_outputs = transformer.decoder(input_ids=dec_ids['input_ids'], \n",
    "                    attention_mask=dec_ids['attention_mask'], \n",
    "                    encoder_hidden_states=flat_enc_hidden_states, \n",
    "                    encoder_attention_mask=flat_enc_atts)\n",
    "sequence_output = dec_outputs[0]\n",
    "lm_logits = transformer.lm_head(sequence_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_set = claim_verification_dataset.data['rule_taker_1']['train'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'Bob is kind.',\n",
       "  'facts': ['Anne is quiet.',\n",
       "   'Dave is round.',\n",
       "   'Fiona is quiet.',\n",
       "   'Dave is rough.',\n",
       "   'Dave is smart.',\n",
       "   'Fiona is not round.',\n",
       "   'Bob is kind.',\n",
       "   'Dave is not young.',\n",
       "   'Anne is not young.',\n",
       "   'Bob is young.',\n",
       "   'Kind, young things are not smart.'],\n",
       "  'answer': True},\n",
       " {'question': 'Dave is not smart.',\n",
       "  'facts': ['Anne is quiet.',\n",
       "   'Dave is round.',\n",
       "   'Fiona is quiet.',\n",
       "   'Dave is rough.',\n",
       "   'Dave is smart.',\n",
       "   'Fiona is not round.',\n",
       "   'Bob is kind.',\n",
       "   'Dave is not young.',\n",
       "   'Anne is not young.',\n",
       "   'Bob is young.',\n",
       "   'Kind, young things are not smart.'],\n",
       "  'answer': False},\n",
       " {'question': 'Bob is not smart.',\n",
       "  'facts': ['Anne is quiet.',\n",
       "   'Dave is round.',\n",
       "   'Fiona is quiet.',\n",
       "   'Dave is rough.',\n",
       "   'Dave is smart.',\n",
       "   'Fiona is not round.',\n",
       "   'Bob is kind.',\n",
       "   'Dave is not young.',\n",
       "   'Anne is not young.',\n",
       "   'Bob is young.',\n",
       "   'Kind, young things are not smart.'],\n",
       "  'answer': True},\n",
       " {'question': 'Bob is smart.',\n",
       "  'facts': ['Anne is quiet.',\n",
       "   'Dave is round.',\n",
       "   'Fiona is quiet.',\n",
       "   'Dave is rough.',\n",
       "   'Dave is smart.',\n",
       "   'Fiona is not round.',\n",
       "   'Bob is kind.',\n",
       "   'Dave is not young.',\n",
       "   'Anne is not young.',\n",
       "   'Bob is young.',\n",
       "   'Kind, young things are not smart.'],\n",
       "  'answer': False},\n",
       " {'question': 'Dave is not quiet.',\n",
       "  'facts': ['Anne is quiet.',\n",
       "   'Dave is round.',\n",
       "   'Fiona is quiet.',\n",
       "   'Dave is rough.',\n",
       "   'Dave is smart.',\n",
       "   'Fiona is not round.',\n",
       "   'Bob is kind.',\n",
       "   'Dave is not young.',\n",
       "   'Anne is not young.',\n",
       "   'Bob is young.',\n",
       "   'Kind, young things are not smart.'],\n",
       "  'answer': True},\n",
       " {'question': 'Anne is rough.',\n",
       "  'facts': ['Anne is quiet.',\n",
       "   'Dave is round.',\n",
       "   'Fiona is quiet.',\n",
       "   'Dave is rough.',\n",
       "   'Dave is smart.',\n",
       "   'Fiona is not round.',\n",
       "   'Bob is kind.',\n",
       "   'Dave is not young.',\n",
       "   'Anne is not young.',\n",
       "   'Bob is young.',\n",
       "   'Kind, young things are not smart.'],\n",
       "  'answer': False},\n",
       " {'question': 'Fiona is not smart.',\n",
       "  'facts': ['Anne is quiet.',\n",
       "   'Dave is round.',\n",
       "   'Fiona is quiet.',\n",
       "   'Dave is rough.',\n",
       "   'Dave is smart.',\n",
       "   'Fiona is not round.',\n",
       "   'Bob is kind.',\n",
       "   'Dave is not young.',\n",
       "   'Anne is not young.',\n",
       "   'Bob is young.',\n",
       "   'Kind, young things are not smart.'],\n",
       "  'answer': True},\n",
       " {'question': 'Dave is kind.',\n",
       "  'facts': ['Anne is quiet.',\n",
       "   'Dave is round.',\n",
       "   'Fiona is quiet.',\n",
       "   'Dave is rough.',\n",
       "   'Dave is smart.',\n",
       "   'Fiona is not round.',\n",
       "   'Bob is kind.',\n",
       "   'Dave is not young.',\n",
       "   'Anne is not young.',\n",
       "   'Bob is young.',\n",
       "   'Kind, young things are not smart.'],\n",
       "  'answer': False},\n",
       " {'question': 'Fiona is white.',\n",
       "  'facts': ['If someone is big and not rough then they are not green.',\n",
       "   'If Fiona is young then Fiona is not big.',\n",
       "   'If someone is white then they are not big.',\n",
       "   'If someone is smart and not young then they are furry.',\n",
       "   'Rough, white people are not young.',\n",
       "   'Fiona is white.',\n",
       "   'If someone is not big then they are not young.'],\n",
       "  'answer': True},\n",
       " {'question': 'Fiona is not white.',\n",
       "  'facts': ['If someone is big and not rough then they are not green.',\n",
       "   'If Fiona is young then Fiona is not big.',\n",
       "   'If someone is white then they are not big.',\n",
       "   'If someone is smart and not young then they are furry.',\n",
       "   'Rough, white people are not young.',\n",
       "   'Fiona is white.',\n",
       "   'If someone is not big then they are not young.'],\n",
       "  'answer': False}]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=  model.to('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Claim: is bob. Answer: false</s> <pad>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.decode([ 7781,   603,    10,    19,     3, 17396,     5, 11801,    10,  6136, 1,     0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input, output and indices must be on the current device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-553cdbd33f7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/nfs/Complex_Decomposition_QA/src/QA_models.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    296\u001b[0m                                    \u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m                                    \u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m                                    use_cache=False)  \n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/Complex_Decomposition_QA/src/QA_models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, fusion_map, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfusion_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mencoder_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfusion_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0mencoder_fused_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mfused_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattentions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/Complex_Decomposition_QA/src/QA_models.py\u001b[0m in \u001b[0;36mencoder_forward\u001b[0;34m(self, fusion_map, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0membed_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfusion_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mencoder_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0mencoder_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, encoder_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"You have to initialize the model with valid token embeddings\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m         return F.embedding(\n\u001b[1;32m    125\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1850\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1852\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input, output and indices must be on the current device"
     ]
    }
   ],
   "source": [
    "model.training_step(batch, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Re-writing: 100%|██████████| 1/1 [00:00<00:00, 34.31it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'facts': ['John is happy.', 'Mary is sad.'],\n",
       "  'decoder_text': '<pad> happy is',\n",
       "  'all_generations': ['<pad> happy is John.</s>'],\n",
       "  'scores': tensor([1.]),\n",
       "  'top_output': '<pad> happy is John.</s>'}]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.inference([{'facts':['John is happy.','Mary is sad.'], 'decoder_text':'<pad> happy is'}], num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/nfs/Complex_Decomposition_QA/src/QA_models.py\u001b[0m(188)\u001b[0;36mprepare_inputs_for_generation\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    186 \u001b[0;31m        \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    187 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 188 \u001b[0;31m        \u001b[0mnew_decoder_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_delta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    189 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    190 \u001b[0;31m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'decoder_attention_mask'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  input_ids\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,  48,  19,   3,   9, 794,  82, 388],\n",
      "        [  0,  48,  19,   3,   9, 794,  82, 388],\n",
      "        [  0,  48,  19,   3,   9, 794,  82, 388]], device='cuda:1')\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  exit\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> La capitale de la capitale de la capitale est paris.'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.decode([   0,    0,    0,    0,    0,    0,    0,    0,  325, 1784,   15,   20,\n",
    "           50, 1784,   15,   20,   50, 1784,   15,  259,  260,  159,    5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "my_model = model.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> La capitale de la capitale est paris.</s>'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_input_ids = t5_tokenizer(\"The capital of <extra_id_0> is paris.\", return_tensors=\"pt\").input_ids  # Batch size 1\n",
    "t5_outputs = t5_model.generate(t5_input_ids, num_beams=3, early_stopping=True, use_cache=False, output_hidden_states=True, return_dict_in_generate=True)\n",
    "t5_tokenizer.decode(t5_outputs.sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-5.6781e-02,  1.2296e-01,  4.2948e-02,  ..., -6.7451e-02,\n",
       "           4.0020e-04,  5.7785e-02],\n",
       "         [-7.0078e-02,  2.4940e-01,  2.6310e-02,  ..., -3.8364e-02,\n",
       "           3.3131e-05,  1.1047e-01]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_outputs.decoder_hidden_states[1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_inputs = t5_tokenizer(\"The capital of <extra_id_0> is Madrid.\", return_tensors=\"pt\")  # Batch size 1\n",
    "my_input_ids = my_inputs.input_ids\n",
    "my_input_att_mask = my_inputs.attention_mask\n",
    "my_decoder_inputs = t5_tokenizer(\"<pad> Madrid,\", return_tensors=\"pt\", add_special_tokens=False)  # Batch size 1\n",
    "my_decoder_ids = my_decoder_inputs.input_ids\n",
    "my_decoder_att_mask = my_decoder_inputs.attention_mask\n",
    "\n",
    "# encoder_outputs = my_model.encoder(my_decoder_input_ids)\n",
    "\n",
    "# t5_outputs = t5_model.generate(t5_input_ids, num_beams=1, early_stopping=True, use_cache=False, output_hidden_states=True, return_dict_in_generate=True)\n",
    "# t5_tokenizer.decode(t5_outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> Madrid,'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_tokenizer.decode([    0, 12033, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12033,     6, 12033]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_model.lm_head(t5_outputs.decoder_hidden_states[2][-1]).argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_fused_states torch.Size([1, 8, 512])\n",
      "fused_attention_mask torch.Size([1, 8])\n",
      "decoder_input_ids torch.Size([1, 3])\n",
      "decoder_attention_mask torch.Size([1, 3])\n",
      "tensor([[[-5.6781e-02,  1.2296e-01,  4.2948e-02,  ..., -6.7451e-02,\n",
      "           4.0020e-04,  5.7785e-02],\n",
      "         [-7.0078e-02,  2.4940e-01,  2.6310e-02,  ..., -3.8364e-02,\n",
      "           3.3131e-05,  1.1047e-01],\n",
      "         [-9.8589e-02,  1.6192e-01,  6.9472e-02,  ..., -9.7110e-02,\n",
      "           4.9539e-04,  1.3432e-01]]], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[12033,     6, 12033]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward([[0,1]], my_input_ids, my_input_att_mask, my_decoder_ids, my_decoder_att_mask).argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=tensor([[[ 1.7982e-01,  6.6164e-02,  4.1064e-02,  ..., -2.1999e-01,\n",
       "           1.0907e-01, -2.3763e-02],\n",
       "         [-3.4724e-01,  1.8860e-01,  9.9379e-02,  ..., -2.6027e-01,\n",
       "          -5.7355e-02, -1.0252e-01],\n",
       "         [ 7.4379e-02, -8.0222e-02,  1.1290e-02,  ...,  2.8214e-02,\n",
       "           5.8411e-02, -5.7466e-02],\n",
       "         ...,\n",
       "         [-1.5884e-01, -3.1136e-01, -8.1628e-02,  ...,  8.8100e-02,\n",
       "           1.1413e-01,  2.0548e-02],\n",
       "         [-2.7359e-02, -2.4308e-01, -5.4980e-02,  ...,  2.5070e-02,\n",
       "          -1.6737e-01, -1.0863e-01],\n",
       "         [ 1.2779e-02, -2.5673e-02, -1.0659e-02,  ..., -7.3198e-02,\n",
       "           5.0401e-03, -6.9694e-05]]], grad_fn=<MulBackward0>), past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model.decoder(my_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0, 32099,  2447,   704, 32098,     8, 32097,  2447,     5,     1])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   37, 32099, 10681,    16, 32098,  2447,     5,     1]])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_input_ids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
